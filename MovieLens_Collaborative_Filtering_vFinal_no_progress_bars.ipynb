{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOlpqT3oQU3KbKppxld9Upi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yckamra/MovieLens25m-Recommender/blob/main/MovieLens_Collaborative_Filtering_vFinal_no_progress_bars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbqC6vrD9IPx",
        "outputId": "7d02c771-4454-4cd2-96f3-813264fd83ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing project dependencies...\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: implicit in /usr/local/lib/python3.11/dist-packages (0.7.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from implicit) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit) (3.6.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: implicit==0.7.2 in /usr/local/lib/python3.11/dist-packages (0.7.2)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from implicit==0.7.2) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from implicit==0.7.2) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit==0.7.2) (3.6.0)\n",
            "Dependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing project dependencies...\")\n",
        "!pip install numpy pandas scipy implicit scikit-learn # Basic libraries we want\n",
        "!pip install --upgrade \"implicit==0.7.2\" \"scipy==1.10.1\"\n",
        "#!pip install torch\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import implicit\n",
        "#import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import coo_matrix, csr_matrix # Compressed Sparse Row\n",
        "from implicit.evaluation import precision_at_k, mean_average_precision_at_k, ndcg_at_k\n",
        "from google.cloud import storage\n",
        "import pickle"
      ],
      "metadata": {
        "id": "nPx3nkGg-MDO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project movielens-recommender-451017\n",
        "\n",
        "!gsutil cp gs://movielens-data/movielens_data.zip /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a47YqbB-Scr",
        "outputId": "13e0a303-79dd-415e-9927-c33f4e6e3b83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://movielens-data/movielens_data.zip...\n",
            "==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "| [1 files][257.5 MiB/257.5 MiB]                                                \n",
            "Operation completed over 1 objects/257.5 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset can be found at: https://www.kaggle.com/datasets/garymk/movielens-25m-dataset"
      ],
      "metadata": {
        "id": "9IeNaqGg-6so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d5dB44X-Vh2",
        "outputId": "ffbd5b60-621f-405c-af37-2a18139a997a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 258M\n",
            "-rw-r--r-- 1 root root 258M Jun 29 23:17 movielens_data.zip\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 13:35 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/movielens_data.zip\"  # Change to your actual zip file name\n",
        "extract_to = \"/content/data/\"  # Where to extract files\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Extraction complete! Files are in:\", extract_to)\n",
        "!ls -lh /content/data/ml-25m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzzp9Zrg-9y_",
        "outputId": "663484e0-7324-4a01-e080-5da15a773812"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete! Files are in: /content/data/\n",
            "total 1.1G\n",
            "-rw-r--r-- 1 root root 416M Jun 29 23:18 genome-scores.csv\n",
            "-rw-r--r-- 1 root root  18K Jun 29 23:18 genome-tags.csv\n",
            "-rw-r--r-- 1 root root 1.4M Jun 29 23:18 links.csv\n",
            "-rw-r--r-- 1 root root 2.9M Jun 29 23:18 movies.csv\n",
            "-rw-r--r-- 1 root root 647M Jun 29 23:18 ratings.csv\n",
            "-rw-r--r-- 1 root root  11K Jun 29 23:18 README.txt\n",
            "-rw-r--r-- 1 root root  38M Jun 29 23:18 tags.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genome_scores_CSV = \"/content/data/ml-25m/genome-scores.csv\"\n",
        "genome_tags_CSV = \"/content/data/ml-25m/genome-tags.csv\"\n",
        "links_CSV = \"/content/data/ml-25m/links.csv\"\n",
        "movies_CSV = \"/content/data/ml-25m/movies.csv\"\n",
        "ratings_CSV = \"/content/data/ml-25m/ratings.csv\"\n",
        "tags_CSV = \"/content/data/ml-25m/tags.csv\""
      ],
      "metadata": {
        "id": "f-4J6ftp-_1C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ratings data\n",
        "ratings_path = ratings_CSV\n",
        "ratings_df = pd.read_csv(ratings_path)\n",
        "\n",
        "print(ratings_df)\n",
        "\n",
        "# Load movies data\n",
        "movies_path = movies_CSV\n",
        "movies_df = pd.read_csv(movies_path)\n",
        "\n",
        "print(movies_df)\n",
        "\n",
        "# Load tags data\n",
        "tags_path = tags_CSV\n",
        "tags_df = pd.read_csv(tags_path)\n",
        "\n",
        "print(tags_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMk3SYhF_Bma",
        "outputId": "a0a0797a-aa5b-4d6c-a05f-e3a1d7770786"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          userId  movieId  rating   timestamp\n",
            "0              1      296     5.0  1147880044\n",
            "1              1      306     3.5  1147868817\n",
            "2              1      307     5.0  1147868828\n",
            "3              1      665     5.0  1147878820\n",
            "4              1      899     3.5  1147868510\n",
            "...          ...      ...     ...         ...\n",
            "25000090  162541    50872     4.5  1240953372\n",
            "25000091  162541    55768     2.5  1240951998\n",
            "25000092  162541    56176     2.0  1240950697\n",
            "25000093  162541    58559     4.0  1240953434\n",
            "25000094  162541    63876     5.0  1240952515\n",
            "\n",
            "[25000095 rows x 4 columns]\n",
            "       movieId                               title  \\\n",
            "0            1                    Toy Story (1995)   \n",
            "1            2                      Jumanji (1995)   \n",
            "2            3             Grumpier Old Men (1995)   \n",
            "3            4            Waiting to Exhale (1995)   \n",
            "4            5  Father of the Bride Part II (1995)   \n",
            "...        ...                                 ...   \n",
            "62418   209157                           We (2018)   \n",
            "62419   209159           Window of the Soul (2001)   \n",
            "62420   209163                    Bad Poems (2018)   \n",
            "62421   209169                 A Girl Thing (2001)   \n",
            "62422   209171      Women of Devil's Island (1962)   \n",
            "\n",
            "                                            genres  \n",
            "0      Adventure|Animation|Children|Comedy|Fantasy  \n",
            "1                       Adventure|Children|Fantasy  \n",
            "2                                   Comedy|Romance  \n",
            "3                             Comedy|Drama|Romance  \n",
            "4                                           Comedy  \n",
            "...                                            ...  \n",
            "62418                                        Drama  \n",
            "62419                                  Documentary  \n",
            "62420                                 Comedy|Drama  \n",
            "62421                           (no genres listed)  \n",
            "62422                       Action|Adventure|Drama  \n",
            "\n",
            "[62423 rows x 3 columns]\n",
            "         userId  movieId                  tag   timestamp\n",
            "0             3      260              classic  1439472355\n",
            "1             3      260               sci-fi  1439472256\n",
            "2             4     1732          dark comedy  1573943598\n",
            "3             4     1732       great dialogue  1573943604\n",
            "4             4     7569     so bad it's good  1573943455\n",
            "...         ...      ...                  ...         ...\n",
            "1093355  162521    66934  Neil Patrick Harris  1427311611\n",
            "1093356  162521   103341     cornetto trilogy  1427311259\n",
            "1093357  162534   189169               comedy  1527518175\n",
            "1093358  162534   189169             disabled  1527518181\n",
            "1093359  162534   189169              robbery  1527518193\n",
            "\n",
            "[1093360 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by user\n",
        "ratings_df.sort_values('userId', inplace=True, ignore_index=True)\n",
        "\n",
        "# Count unique users and movies\n",
        "num_users = ratings_df[\"userId\"].nunique()\n",
        "num_movies = ratings_df[\"movieId\"].nunique()\n",
        "\n",
        "print(f\"Number of Unique Users: {num_users}\")\n",
        "print(f\"Number of Unique Movies: {num_movies}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL8tVHSu_DhS",
        "outputId": "40a5113f-88a1-421a-d12a-e6f2f4f413e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Unique Users: 162541\n",
            "Number of Unique Movies: 59047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using collaborative filtering for recommendations. A 2D matrix will be created for movie to user ratings with movies on x-axis and users on y-axis. For new users, we ask for a baseline amount of movies to get a starting vector, and then we use cosine similarity to find their similarity to existing users to recommend them movies. With more ratings, comes greater accuracy."
      ],
      "metadata": {
        "id": "XWtY0wP-_KCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#numpy_matrix = np.zeros((num_users, num_movies))\n",
        "#print(numpy_matrix)\n",
        "#print(numpy_matrix.shape)"
      ],
      "metadata": {
        "id": "1fNDYf7H_Fzg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to map IDs of movies and users to the 2D matrix for collaborative filtering, we will use dictionaries to create key-value pairs between ID and index within the matrix."
      ],
      "metadata": {
        "id": "qRula-uR_QN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id_map = {}\n",
        "movie_id_map = {}\n",
        "user_index = 0\n",
        "movie_index = 0\n",
        "\n",
        "# Create user_id and movie_id to 2D matrix indices\n",
        "for user in ratings_df[\"userId\"]: # Use this for sparse matrix, not userId since users could be removed from dataset\n",
        "  if user not in user_id_map:\n",
        "    user_id_map[user] = user_index\n",
        "    user_index += 1\n",
        "\n",
        "for movie in ratings_df[\"movieId\"]:\n",
        "  if movie not in movie_id_map:\n",
        "    movie_id_map[movie] = movie_index\n",
        "    movie_index += 1"
      ],
      "metadata": {
        "id": "pLZQI8nM_MX-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice below that we are populating a dense matrix, so if we have ~25 million cells to populate out of the ~9 billion from our 162,541 x 59047 matrix filled with zeroes, we are dealing with major sparcity problems and wasted memory. So below is just to later enjoy shorter train times and smaller matrices. Going forward we will be using a Compressed Sparse Row matrix. This will help in learning user and movie abstract feature matrices and cosine similarities."
      ],
      "metadata": {
        "id": "I8aiTJ0u_U7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.display import clear_output\n",
        "\n",
        "#iter = 0\n",
        "#percent_done = 0\n",
        "#total_rows = len(ratings_df)\n",
        "# Iterate over the rows of the DataFrame\n",
        "#for index, row in ratings_df.iterrows():\n",
        "    #user_index = user_id_map[row[\"userId\"]]\n",
        "    #movie_index = movie_id_map[row[\"movieId\"]]\n",
        "    #numpy_matrix[user_index, movie_index] = row[\"rating\"]\n",
        "    #if iter % int(total_rows / 100) == 0:\n",
        "      #clear_output(wait=True)\n",
        "      #print(f\"Percent of matrix populated with ratings: {percent_done}%\")\n",
        "      #percent_done += 1\n",
        "    #iter += 1"
      ],
      "metadata": {
        "id": "PoO4eIgp_Shu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ratings_dataframe = pd.DataFrame(numpy_matrix)\n",
        "#print(ratings_dataframe.head())"
      ],
      "metadata": {
        "id": "pO4wSwmO_XX2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing the dataframe ratings to the original Kaggle dataset\n",
        "#print(f\"Original Rating: 4.0 Found Rating: {ratings_dataframe.loc[user_id_map[1], movie_id_map[8786]]}\")\n",
        "#print(f\"Original Rating: 2.0 Found Rating: {ratings_dataframe.loc[user_id_map[2], movie_id_map[480]]}\")\n",
        "#print(f\"Original Rating: 3.5 Found Rating: {ratings_dataframe.loc[user_id_map[3], movie_id_map[1270]]}\")\n",
        "#print(ratings_dataframe.shape)"
      ],
      "metadata": {
        "id": "mtbVaoHM_Y2B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I have created my own arrays for sparse matrix. We will use scipy in order to have more accurate conversions, but this gives us another option and removes the blackbox."
      ],
      "metadata": {
        "id": "J_GlnwD2_bUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_data_array(df):  # Numpy array of ratings\n",
        "  ratings_df = df\n",
        "  data = ratings_df[\"rating\"].to_numpy()\n",
        "  print(data)\n",
        "  return data\n",
        "\n",
        "def calculate_movie_indices(df):   # Numpy array of movie indices\n",
        "  ratings_df = df\n",
        "  indices = ratings_df[\"movieId\"].to_numpy()\n",
        "\n",
        "  for i in range(len(indices)):\n",
        "      indices[i] = movie_id_map[int(indices[i])]\n",
        "  print(indices)\n",
        "  return indices\n",
        "\n",
        "\n",
        "def calculate_indptr(df):   # Numpy array of pointers to indices\n",
        "  ratings_df = df\n",
        "  num_users = ratings_df[\"userId\"].nunique()\n",
        "  indptr = np.zeros(num_users + 1, dtype=int)\n",
        "  last_user = ratings_df[\"userId\"].iloc[0]\n",
        "  index_in_indptr = 1\n",
        "  iterator = 0\n",
        "\n",
        "  for i in range(len(ratings_df[\"userId\"])):\n",
        "    current_user = ratings_df[\"userId\"].iloc[iterator]\n",
        "    if current_user != last_user:\n",
        "      indptr[index_in_indptr] = iterator\n",
        "      index_in_indptr += 1\n",
        "      last_user = current_user\n",
        "    iterator += 1\n",
        "\n",
        "  indptr[index_in_indptr] = len(ratings_df[\"userId\"]) # This fills up the last indice\n",
        "\n",
        "  print(indptr)\n",
        "  return indptr\n",
        "\n",
        "#data = {'userId': [0, 0, 1, 2, 2, 2, 3]} # For testing\n",
        "#df = pd.DataFrame(data) # For testing\n",
        "#calculate_indptr(df) # For testing"
      ],
      "metadata": {
        "id": "yowUYRs0_at3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the maps to the DataFrame columns to get the 0-indexed row and column arrays\n",
        "row_indices = ratings_df['userId'].map(user_id_map).to_numpy()\n",
        "col_indices = ratings_df['movieId'].map(movie_id_map).to_numpy()\n",
        "data_values = ratings_df['rating'].to_numpy() # The actual ratings\n",
        "\n",
        "# Master COO matrix\n",
        "user_item_matrix_coo = coo_matrix(\n",
        "    (data_values, (row_indices, col_indices)),\n",
        "    shape=(num_users, num_movies)\n",
        ")\n",
        "\n",
        "# Master CSR matrix\n",
        "user_item_matrix_csr = user_item_matrix_coo.tocsr()\n",
        "\n",
        "# Split into train and test sets\n",
        "#(train_row_indices, test_row_indices,\n",
        "# train_col_indices, test_col_indices,\n",
        "# train_data_values, test_data_values) = train_test_split(\n",
        "#    row_indices,\n",
        "#    col_indices,\n",
        "#    data_values,\n",
        "#    test_size=0.3,\n",
        "#    random_state=42\n",
        "#)\n",
        "\n",
        "#print(train_row_indices)\n",
        "\n",
        "# Create the train COO Matrix\n",
        "#train_matrix_coo = coo_matrix(\n",
        "#    (train_data_values, (train_row_indices, train_col_indices)),\n",
        "#    shape=(num_users, num_movies)\n",
        "#)\n",
        "\n",
        "# Convert to CSR (Compressed Sparse Row) for the training set\n",
        "#train_matrix_csr = train_matrix_coo.tocsr()\n",
        "\n",
        "# Create the test COO Matrix\n",
        "#test_matrix_coo = coo_matrix(\n",
        "#    (test_data_values, (test_row_indices, test_col_indices)),\n",
        "#    shape=(num_users, num_movies)\n",
        "#)\n",
        "\n",
        "# Convert to CSR (Compressed Sparse Row) for the test set\n",
        "#test_matrix_csr = test_matrix_coo.tocsr()\n",
        "\n",
        "# Stack the row, col, value into tuples\n",
        "interactions = list(zip(row_indices, col_indices, data_values))\n",
        "\n",
        "# Use train_test_split on the full triplets\n",
        "train_data, test_data = train_test_split(interactions, test_size=0.3, random_state=42)\n",
        "\n",
        "# Separate the triplets again for matrix construction\n",
        "train_rows, train_cols, train_vals = zip(*train_data)\n",
        "test_rows, test_cols, test_vals = zip(*test_data)\n",
        "\n",
        "# Build sparse matrices\n",
        "train_matrix_csr = coo_matrix((train_vals, (train_rows, train_cols)), shape=(num_users, num_movies)).tocsr()\n",
        "test_matrix_csr = coo_matrix((test_vals, (test_rows, test_cols)), shape=(num_users, num_movies)).tocsr()"
      ],
      "metadata": {
        "id": "pc5wPU-7_ffF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice below that the indice array is different than our function. This is because although the user indice array is sorted as the main sort, within each user's range--the subsection for each user--has their movie indices sorted as well."
      ],
      "metadata": {
        "id": "2KdK92ky_lQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Converted to CSR Matrix.\")\n",
        "print(\"Train CSR Shape:\", train_matrix_csr.shape)\n",
        "print(\"Train CSR number of examples:\", train_matrix_csr.nnz)\n",
        "print(\"Train CSR data:\", train_matrix_csr.data)\n",
        "print(\"Train CSR indices (column indices):\", train_matrix_csr.indices)\n",
        "print(\"Train CSR indptr (pointers to row starts):\", train_matrix_csr.indptr)\n",
        "print(\"Test CSR Shape:\", test_matrix_csr.shape)\n",
        "print(\"Test CSR number of examples:\", test_matrix_csr.nnz)\n",
        "print(\"Test CSR data:\", test_matrix_csr.data)\n",
        "print(\"Test CSR indices (column indices):\", test_matrix_csr.indices)\n",
        "print(\"Test CSR indptr (pointers to row starts):\", test_matrix_csr.indptr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf5a7hSO_ig2",
        "outputId": "e11218cf-588a-41fb-9ca5-0751e9c22597"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted to CSR Matrix.\n",
            "Train CSR Shape: (162541, 59047)\n",
            "Train CSR number of examples: 17500066\n",
            "Train CSR data: [5.  5.  3.5 ... 2.  3.  2.5]\n",
            "Train CSR indices (column indices): [    0     3     4 ... 12213 12780 14125]\n",
            "Train CSR indptr (pointers to row starts): [       0       55      188 ... 17499873 17499937 17500066]\n",
            "Test CSR Shape: (162541, 59047)\n",
            "Test CSR number of examples: 7500029\n",
            "Test CSR data: [2.5 4.  4.  ... 4.  4.  3.5]\n",
            "Test CSR indices (column indices): [   1    2   12 ... 3845 5693 6221]\n",
            "Test CSR indptr (pointers to row starts): [      0      15      66 ... 7499952 7499976 7500029]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cosine_similarity_userX_to_allUsers(userId, user_id_map, user_item_matrix_csr): # TODO: we need to remove the own user from its similarity array\n",
        "  user_index = user_id_map[userId]\n",
        "  target_user_vector = user_item_matrix_csr[user_index : user_index + 1]\n",
        "  return cosine_similarity(target_user_vector, user_item_matrix_csr)"
      ],
      "metadata": {
        "id": "vBHKosf5_nd8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_cosine_similarity_userX_to_allUsers(1, user_id_map, train_matrix_csr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONeGmd4X_pQu",
        "outputId": "265893b8-c10b-415b-e0f0-2eb9e4579613"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.04059364 0.03063487 ... 0.         0.01862104 0.06664523]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model for matrix factorization will be Alternating Least Squares (ALS)."
      ],
      "metadata": {
        "id": "XTCxqkUR_vDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_factors = 50       # Number of dimensions to learn\n",
        "regularization = 0.01  # L2 regularization\n",
        "num_iterations = 20    # Number of training iterations\n",
        "\n",
        "# ALS model\n",
        "matrix_factorization_model = implicit.als.AlternatingLeastSquares(\n",
        "    factors=num_factors,\n",
        "    regularization=regularization,\n",
        "    iterations=num_iterations,\n",
        "    random_state=42,\n",
        "    calculate_training_loss=True # Monitors loss\n",
        ")\n",
        "\n",
        "# Model will learn confidence of a user's interest in a movie and will not predict scores (0.5-5.0)\n",
        "# Larger confidence score, the more likely the user will like the movie\n",
        "matrix_factorization_model.fit(train_matrix_csr)\n",
        "\n",
        "# Store our user and movie abstract feature (latent) matrices\n",
        "user_factors = matrix_factorization_model.user_factors # Shape is num_users by num_factors\n",
        "item_factors = matrix_factorization_model.item_factors # Shape is num_items by num_factors\n",
        "\n",
        "print(\"ALS model training complete!\")\n",
        "print(f\"Learned User Factors Shape: {user_factors.shape}\")\n",
        "print(f\"Learned Movie Factors Shape: {item_factors.shape}\")"
      ],
      "metadata": {
        "id": "FBTGHmBb_qy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1:\n",
        "*   50 factors, 0.01 regularization, 20 num of iterations\n",
        "*   training loss: 0.00301\n",
        "*   Precision@20: 0.4536\n",
        "*   MAP@20: 0.2945\n",
        "*   NDCG@20: 0.4506\n",
        "\n",
        "Model 2:\n",
        "*   100 factors, 0.01 regularization, 20 num of iterations\n",
        "*   0.00278 loss on train\n",
        "*   Precision@20: 0.4388\n",
        "*   MAP@20: 0.2772\n",
        "*   NDCG@20: 0.4359\n"
      ],
      "metadata": {
        "id": "K9q4vOwn_z6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = 20\n",
        "\n",
        "# Evaluate\n",
        "precision = precision_at_k(matrix_factorization_model, train_matrix_csr, test_matrix_csr, K=K)\n",
        "map_k = mean_average_precision_at_k(matrix_factorization_model, train_matrix_csr, test_matrix_csr, K=K)\n",
        "ndcg = ndcg_at_k(matrix_factorization_model, train_matrix_csr, test_matrix_csr, K=K)\n",
        "\n",
        "print(f\"Precision@{K}: {precision:.4f}\")\n",
        "print(f\"MAP@{K}: {map_k:.4f}\")\n",
        "print(f\"NDCG@{K}: {ndcg:.4f}\")"
      ],
      "metadata": {
        "id": "7klml2Hv_xWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_gcs(thing_to_save, bucket_name: str, destination_blob_name: str):\n",
        "    try:\n",
        "        auth.authenticate_user()\n",
        "        print(\"Authenticated to Google Cloud.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not authenticate user (may be running outside Colab or already authenticated): {e}\")\n",
        "\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Pickle the object into a bytes object\n",
        "    pickled_data = pickle.dumps(thing_to_save)\n",
        "\n",
        "    # Upload object to GCS\n",
        "    blob.upload_from_string(pickled_data, content_type='application/octet-stream')\n",
        "\n",
        "    print(f\"Object successfully saved to gs://{bucket_name}/{destination_blob_name}\")"
      ],
      "metadata": {
        "id": "kMdXSgQF_24F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"movielens-data\"\n",
        "\n",
        "blob_name = \"collaborative_filtering/user_id_map.pkl\"\n",
        "save_to_gcs(user_id_map, bucket_name, blob_name)\n",
        "\n",
        "blob_name = \"collaborative_filtering/movie_id_map.pkl\"\n",
        "save_to_gcs(movie_id_map, bucket_name, blob_name)\n",
        "\n",
        "blob_name = \"collaborative_filtering/user_item_matrix_csr.pkl\"\n",
        "save_to_gcs(user_item_matrix_csr, bucket_name, blob_name)\n",
        "\n",
        "blob_name = \"collaborative_filtering/train_matrix_csr.pkl\"\n",
        "save_to_gcs(train_matrix_csr, bucket_name, blob_name)\n",
        "\n",
        "blob_name = \"collaborative_filtering/test_matrix_csr.pkl\"\n",
        "save_to_gcs(test_matrix_csr, bucket_name, blob_name)\n",
        "\n",
        "blob_name = \"collaborative_filtering/matrix_factorization_model.pkl\"\n",
        "save_to_gcs(matrix_factorization_model, bucket_name, blob_name)"
      ],
      "metadata": {
        "id": "zLVpqFMs_5-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}